{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Activation, SimpleRNN\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from keras.utils import np_utils, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_file(filename):\n",
    "    X_2 = []\n",
    "    y_2 = []\n",
    "    X_5 = []\n",
    "    y_5 = []\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            index = 0\n",
    "            indices = []\n",
    "            for char in line:\n",
    "                if char == ',':\n",
    "                    indices.append(index)\n",
    "                    break\n",
    "                index += 1\n",
    "            value = line[0:index]\n",
    "            tag = ''\n",
    "\n",
    "            if float(value) > 0.8:\n",
    "                tag = 4\n",
    "            elif float(value) > 0.6:\n",
    "                tag = 3\n",
    "            elif float(value) > 0.4:\n",
    "                tag = 2\n",
    "            elif float(value) > 0.2:\n",
    "                tag = 1\n",
    "            else:\n",
    "                tag = 0\n",
    "            y_5.append(tag)\n",
    "            X_5.append(\" \".join(tokenizer.tokenize(line[index + 1:])))\n",
    "\n",
    "\n",
    "            if float(value) > 0.4 and float(value) <= 0.6:\n",
    "                continue\n",
    "\n",
    "            if float(value) > 0.6:\n",
    "                tag = 1\n",
    "            elif float(value) <= 0.4:\n",
    "                tag = 0\n",
    "            y_2.append(tag)\n",
    "            X_2.append(\" \".join(tokenizer.tokenize(line[index + 1:])))\n",
    "\n",
    "\n",
    "    return X_2, X_5, y_2, y_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVocab = {}\n",
    "\n",
    "def createVocab():\n",
    "    vocab = {}\n",
    "    count = 0\n",
    "    with open(\"sst_train_sentences.csv\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            index = 0\n",
    "            for char in line:\n",
    "                if char == ',':\n",
    "                    break\n",
    "                index += 1\n",
    "            words = tokenizer.tokenize(line[index + 1:])\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                fdist[word] += 1\n",
    "    with open(\"sst_dev.csv\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            index = 0\n",
    "            for char in line:\n",
    "                if char == ',':\n",
    "                    break\n",
    "                index += 1\n",
    "            words = tokenizer.tokenize(line[index + 1:])\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                fdist[word] += 1\n",
    "    with open(\"sst_test.csv\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            index = 0\n",
    "            for char in line:\n",
    "                if char == ',':\n",
    "                    break\n",
    "                index += 1\n",
    "            words = tokenizer.tokenize(line[index + 1:])\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                fdist[word] += 1\n",
    "    file = open(\"wordVocab.txt\", 'w')\n",
    "    count = 1\n",
    "    for word,value in fdist.most_common():\n",
    "        file.write(str(word) + \":\" + str(count) + \"\\n\")\n",
    "        count = count +1\n",
    "    file.close()\n",
    "\n",
    "def loadVocab():\n",
    "    file = open(\"wordVocab.txt\")\n",
    "    for line in file:\n",
    "        value = line.split(\":\")\n",
    "        wordVocab[value[0]] = int(value[1].strip())\n",
    "\n",
    "def sentence_to_ids(sentence):\n",
    "    ids = []\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word != \"\":\n",
    "            ids.append(wordVocab[word])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating\n",
      "loading\n"
     ]
    }
   ],
   "source": [
    "print(\"creating\")\n",
    "createVocab()\n",
    "print(\"loading\")\n",
    "loadVocab()\n",
    "embedding_size = 100\n",
    "vocabulary_size = len(wordVocab) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_lstm(X_train, y_train,X_valid, y_valid, X_test, y_test, batch_size,neurons, dropout, reg_value, nb_class, writer, nb_layer, rec_drop, max_length):\n",
    "    print([batch_size,neurons, dropout, reg_value, nb_layer, rec_drop])\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_size, input_length=max_length))\n",
    "    neuron_nb = neurons\n",
    "    for i in range(nb_layer):\n",
    "        if i == nb_layer -1:\n",
    "            model.add(\n",
    "                LSTM(neuron_nb, return_sequences=False, dropout=rec_drop, recurrent_dropout=rec_drop))\n",
    "        else:\n",
    "            model.add(\n",
    "                LSTM(neuron_nb, return_sequences=True, dropout=rec_drop, recurrent_dropout=rec_drop))\n",
    "        neuron_nb = int(neuron_nb / 2)\n",
    "        if neuron_nb == 0:\n",
    "            neuron_nb = 1\n",
    "            \n",
    "    model.add(Dropout(dropout))\n",
    "    if nb_class == 2:\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(5, activation=\"softmax\"))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    acc = []\n",
    "    val_acc = []\n",
    "    y = []\n",
    "    best_valid_acc = 0\n",
    "    num_under = 0\n",
    "    epoch_nb = 0\n",
    "    while epoch_nb < 2:\n",
    "        y.append(epoch_nb)\n",
    "        train_history = model.fit(X_train, y_train,validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=1, shuffle=True)\n",
    "        model.reset_states()\n",
    "        print(\"epoch \" +str(epoch_nb))\n",
    "        acc.append(train_history.history[\"acc\"][0])\n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        val_acc.append(scores[1])\n",
    "        valid_loss = train_history.history[\"val_loss\"][0]\n",
    "        valid_acc = train_history.history[\"val_acc\"][0]\n",
    "        print('Test accuracy:', scores[1])\n",
    "        print(\"Test loss: \", scores[0])\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "        writer.writerow([epoch_nb, batch_size,neurons, dropout, reg_value, rec_drop, nb_layer, value_loss, value_acc, scores[0], scores[1], max_length])\n",
    "        epoch_nb += 1\n",
    "        return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 25, 0.6, 1e-05, 1, 0.2]\n",
      "Epoch 1/1\n",
      "7783/7783 [==============================] - 22s 3ms/step - loss: 0.2144 - acc: 0.6522\n",
      "epoch 0\n",
      "Test accuracy: 0.7545304776612773\n",
      "Test loss:  0.16455615019320394\n",
      "Epoch 1/1\n",
      "7783/7783 [==============================] - 21s 3ms/step - loss: 0.1097 - acc: 0.8583\n",
      "epoch 1\n",
      "Test accuracy: 0.8226249314218612\n",
      "Test loss:  0.13415773762804278\n"
     ]
    }
   ],
   "source": [
    "## to train 2 classification\n",
    "\n",
    "\n",
    "file = open(\"rnn_results_coarse_grained.csv\", 'w')\n",
    "writer= csv.writer(file)\n",
    "writer.writerow([\"epoch_nb\", \"batch_size\",\"neurons_nb\", \"dropout_value\", \"reg_value\",\"recurrent_reg present\",  \"nb_layer\",\"current_loss\",\"current_accuracy\",\n",
    "             \"test_loss\", \"test_accuracy\"])\n",
    "count = 0\n",
    "for batch_size in [30]:\n",
    "    for dropout_value in [0.6]:\n",
    "        for nb_layer in [1]:\n",
    "            for reg_value in [1e-5]:\n",
    "                for rec_drop in [0.2]:\n",
    "                    for neuron_nb in [25]:\n",
    "                        for max_length in [35]:\n",
    "\n",
    "                            X_train_2, X_train_5, y_train_2, y_train_5 = load_file('sst_train_sentences.csv')\n",
    "                            X_dev_2, X_dev_5, y_dev_2, y_dev_5 = load_file('sst_dev.csv')\n",
    "                            X_test_2, X_test_5, y_test_2, y_test_5 = load_file('sst_test.csv')\n",
    "\n",
    "                            \n",
    "                            X_train_ids_2 = []\n",
    "                            X_test_ids_2 = []\n",
    "                            X_dev_ids_2 = []\n",
    "                            X_train_ids_5 = []\n",
    "                            X_test_ids_5 = []\n",
    "                            X_dev_ids_5 = []\n",
    "\n",
    "                            for i in range(len(X_train_2)):\n",
    "                                X_train_ids_2.append(sentence_to_ids(X_train_2[i]))\n",
    "                            for i in range(len(X_test_2)):\n",
    "                                X_test_ids_2.append(sentence_to_ids(X_test_2[i]))\n",
    "                            for i in range(len(X_dev_2)):\n",
    "                                X_dev_ids_2.append(sentence_to_ids(X_dev_2[i]))\n",
    "\n",
    "\n",
    "                            X_train_ids_2 = np.array(X_train_ids_2)\n",
    "                            X_test_ids_2 = np.array(X_test_ids_2)\n",
    "                            X_dev_ids_2 = np.array(X_dev_ids_2)\n",
    "                            y_train_2 = np.array(y_train_2)\n",
    "                            y_dev_2 = np.array(y_dev_2)\n",
    "                            y_test_2 = np.array(y_test_2)\n",
    "\n",
    "                            X_train_ids_2 = sequence.pad_sequences(X_train_ids_2, maxlen=max_length)\n",
    "                            X_test_ids_2 = sequence.pad_sequences(X_test_ids_2, maxlen=max_length)\n",
    "                            X_dev_ids_2 = sequence.pad_sequences(X_dev_ids_2, maxlen=max_length)\n",
    "\n",
    "\n",
    "                            X_train_2 = np.empty((len(X_train_ids_2) + len(X_dev_ids_2), max_length))\n",
    "                            y_train_final_2 = np.empty((len(X_train_ids_2) + len(X_dev_ids_2), 1))\n",
    "\n",
    "                            for i in range(len(X_train_ids_2)):\n",
    "                                X_train_2[i] = X_train_ids_2[i]\n",
    "                                y_train_final_2[i] = y_train_2[i]\n",
    "                            for i in range(len(X_dev_ids_2)):\n",
    "                                X_train_2[len(X_train_ids_2) + i] = X_dev_ids_2[i]\n",
    "                                y_train_final_2[len(X_train_ids_2) + i] = y_dev_2[i]\n",
    "\n",
    "\n",
    "                            valid_fit_lstm(X_train_2 , y_train_final_2, X_test_ids_2, y_test_2, batch_size, neuron_nb, dropout_value, reg_value, 2, writer, nb_layer, rec_drop, max_length)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 30, 0.7, 1e-05, 1, 0.2]\n",
      "Epoch 1/1\n",
      "9634/9634 [==============================] - 44s 5ms/step - loss: 1.5665 - acc: 0.2880\n",
      "epoch 0\n",
      "Test accuracy: 0.35429864253393667\n",
      "Test loss:  1.4844735269632814\n",
      "Epoch 1/1\n",
      " 210/9634 [..............................] - ETA: 32s - loss: 1.4702 - acc: 0.3762"
     ]
    }
   ],
   "source": [
    "\n",
    "file = open(\"rnn_results_fine_grained.csv\", 'w')\n",
    "writer= csv.writer(file)\n",
    "writer.writerow([\"epoch_nb\", \"batch_size\",\"neurons_nb\", \"dropout_value\", \"reg_value\",\"recurrent_reg present\",  \"nb_layer\",\"current_loss\",\"current_accuracy\",\n",
    "             \"test_loss\", \"test_accuracy\"])\n",
    "count = 0\n",
    "for max_length in [45]:\n",
    "    X_train_2, X_train_5, y_train_2, y_train_5 = load_file('sst_train_sentences.csv')\n",
    "    X_dev_2, X_dev_5, y_dev_2, y_dev_5 = load_file('sst_dev.csv')\n",
    "    X_test_2, X_test_5, y_test_2, y_test_5 = load_file('sst_test.csv')\n",
    "\n",
    "    #\n",
    "    X_train_ids_2 = []\n",
    "    X_test_ids_2 = []\n",
    "    X_dev_ids_2 = []\n",
    "    X_train_ids_5 = []\n",
    "    X_test_ids_5 = []\n",
    "    X_dev_ids_5 = []\n",
    "\n",
    "    for i in range(len(X_train_5)):\n",
    "        X_train_ids_5.append(sentence_to_ids(X_train_5[i]))\n",
    "    for i in range(len(X_test_5)):\n",
    "        X_test_ids_5.append(sentence_to_ids(X_test_5[i]))\n",
    "    for i in range(len(X_dev_5)):\n",
    "        X_dev_ids_5.append(sentence_to_ids(X_dev_5[i]))\n",
    "\n",
    "    X_train_ids_5 = np.array(X_train_ids_5)\n",
    "    X_test_ids_5 = np.array(X_test_ids_5)\n",
    "    X_dev_ids_5 = np.array(X_dev_ids_5)\n",
    "    y_train_5 = np.array(y_train_5)\n",
    "    y_dev_5 = np.array(y_dev_5)\n",
    "    y_test_5 = np.array(y_test_5)\n",
    "\n",
    "    X_train_ids_5 = sequence.pad_sequences(X_train_ids_5, maxlen=max_length)\n",
    "    X_test_ids_5 = sequence.pad_sequences(X_test_ids_5, maxlen=max_length)\n",
    "    X_dev_ids_5 = sequence.pad_sequences(X_dev_ids_5, maxlen=max_length)\n",
    "\n",
    "    X_train_5 = np.empty((len(X_train_ids_5) + len(X_dev_ids_5), max_length))\n",
    "    y_train_final_5 = np.empty((len(X_train_ids_5) + len(X_dev_ids_5), 1))\n",
    "\n",
    "    for i in range(len(X_train_ids_5)):\n",
    "        X_train_5[i] = X_train_ids_5[i]\n",
    "        y_train_final_5[i] = y_train_5[i]\n",
    "    for i in range(len(X_dev_ids_5)):\n",
    "        X_train_5[len(X_train_ids_5) + i] = X_dev_ids_5[i]\n",
    "        y_train_final_5[len(X_train_ids_5) + i] = y_dev_5[i]\n",
    "\n",
    "    y_train_final_5 = to_categorical(y_train_final_5)\n",
    "    y_test_5 = to_categorical(y_test_5)\n",
    "\n",
    "    for batch_size in [30]:\n",
    "        for dropout_value in [0.7]:\n",
    "            for nb_layer in [1]:\n",
    "                for reg_value in [1e-5]:\n",
    "                    for rec_drop in [0.2]:\n",
    "                        for neuron_nb in [30]:\n",
    "                            fit_lstm(X_train_5 , y_train_final_5, X_test_ids_5, y_test_5, batch_size, neuron_nb, dropout_value, reg_value, 5, writer, nb_layer, rec_drop, max_length)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
